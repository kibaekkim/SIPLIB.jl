\documentclass[11pt,english]{article}
%\documentclass[oneside, english]{article}

\usepackage[numbers,comma]{natbib}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
%\usepackage{cite}     
\ifx\pdfoutput\undefined
\usepackage{graphicx}
\else
\usepackage[pdftex]{graphicx}
\fi
\usepackage{psfrag}    
\usepackage{subfigure} 
\usepackage{url}       
\usepackage{stfloats}  
\usepackage{amssymb,amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{mathrsfs}
\usepackage{appendix}
\usepackage{setspace}
%\renewcommand{\baselinestretch}{2}
\usepackage[top=1in, bottom=1in, left=1in, right=1in]{geometry} 
\usepackage{multirow}
\usepackage{xcolor}
\usepackage{tabularx,latexsym}

%\newtheorem{theorem}{Theorem}[section]
%\newtheorem{corollary}[theorem]{Corollary}
%\newtheorem{lemma}[theorem]{Lemma}
%\newtheorem{proposition}[theorem]{Proposition}
%\newtheorem{remark}{Remark}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{claim}{Claim}
\newtheorem{conjecture}{Conjecture}
\newtheorem{hypothesis}{Hypothesis}
\newtheorem{assumption}{Assumption}
\newtheorem{remark}{Remark}
\newtheorem{example}{Example}
\newtheorem{problem}{Problem}
\newtheorem{definition}{Definition}
\newtheorem{question}{Question}
\newtheorem{answer}{Answer}
\newtheorem{exercise}{Exercise}

\makeatletter
\usepackage{babel}
\makeatother

\setlength{\parindent}{0mm}
%\doublespacing
\begin{document}
{\Large \textbf{Note on 2/16/18}}\\
\linebreak
\textbf{Breakdown of MIPLIB 2010: Contents \& Summary} \textcolor{blue}{(my opinion\&questions in blue color)}
\begin{enumerate}
	\item Introduction \\
	They describes what they have done via MIPLIB 2010: provided instances, categories of instances (easy, hard, open), progress in solving MIP instances, speedup of commercial MIP solvers, how much it becomes faster to solve MIPs, breakdown of the instances
	\item The test sets (361 in total)\\
	MIPLIB 2003 contains only 60 instances$\Rightarrow$Inadequate test sets for researchers. The authors identified ``several areas'' for which dedicated test sets should be made available:
	\begin{itemize}
		\item Benchmark (87): instances solvable to opt. within $\le$2 hours on high-end PC
		\item Infeasible (20): infeasible instances \textcolor{blue}{(are these useful?).}
		\item Primal (40): instances for which the solution of the root LP relaxation has the same objective value as the optimal solution \textcolor{blue}{(why the name ``primal''?)}.
		\item XXL (11): very large instances w.r.t. \#variables, \#constraints, \#non-zeros.
		\item Reoptimize (66): instances for which the reoptimization of the sub-LPs takes an unusually long time.
		\item Tree (52): instances that lead to large enumeration trees \textcolor{blue}{(how they find/measure it? solvers provide information?)}
		\item Unstable (21): instances that have ``bad''\textcolor{blue}{(?)} numerical properties and are likely to cause numerical troubles \textcolor{blue}{(?)} in the solver. This set is intended to test solver robustness. 
		\item Challenge (164): compliation of hard-to-solve instances, instances that to our knowledge have not been solved to optimality. For 21 instances, no feasible solution has been found. Some of these may be infeasible.
	\end{itemize}
	\begin{enumerate}
		\item What are the sources of the instances?\\
		They started a call for instances: March 2010-Oct. 2010. 1,108 instances from 57 contributors. They translated them to .MPS format. When somebody sent .AMPL model, they preserved them on MIPLIB website. And any other internet repositories: BCOL, CORL, DEIS-ORGLI, etc. Initially, the number of candidate set contained about 2,000 instances. \textcolor{blue}{(I think, we also need to make a  call for instances to researchers. Prior to that, we need to make a ``format'' in which they can submit instances. The format can roughly contain: the form of SIP model, application type - energy, manufacturing, logistics, telecom \#scenarios, prob. for each scenario, etc. We will provide a sample format for each family of instances so that they can refer.)}
		\item How were the instances selected?
		\begin{itemize}
			\item Exclusion of trivial instances and (near) duplicates.
			\item Examination and pre-selection by eight groups.
			\item Final refinement of the benchmark set.
		\end{itemize}
	\end{enumerate}
	\item The solution checker
	\begin{enumerate}
		\item Floating-point arithmetic and tolerances
		\item What do MIP solvers actually try to solve?
		\item What does the solution checker test?
	\end{enumerate}
	\item How to run a test, add a solver, and what the scripts do?
	\item Variability of MIP solver performances
	\begin{enumerate}
		\item Reasons for performance variability
		\item Generating and measureing performance variability
		\item Results on performance variability
		\item Consequences for benchmarking
	\end{enumerate}
	\item The instance catalog
	\item State-of-the-art MIP solving
	\item Final remarks
\end{enumerate}
	
\newpage
{\Large \textbf{Meeting 11/06/09}}\\
\linebreak
\textbf{Formulation} \\
\linebreak
\underline{Given:}\\
$A$: set of applications\\
$S$: set of servers\\
$T$: set of time slots (e.g. every hour of a week)\\
$\lambda_{it}$: average arrival rate of application $i$ at time $t$ for all $i \in A$, $t \in T$\\
$\forall i \in A$, $b_i \equiv$ memory required for application $i$\\
$\forall j \in S$, $c_i \equiv$ memory available on server $j$\\
$\forall j \in S$, $\mu_j \equiv$ maximum service rate of server $j$\\
$\mu_{ij}$: average service rate for application $i$ when it ran at maximum speed in server $j$ for $i \in A$, $j\in S$\\
$\underbrace{\rho \equiv\textrm{: target load (max) }}_{\textrm{surrogate for QoS}}$\\
\linebreak
\underline{Decision variables}\\
$\forall i\in A, j\in S$\\
$  x_{ij}= \left\{ \begin{array}{ll}
      1 & \textrm{if application $i$ is assigned to server $j$} \\
      0 & \textrm{otherwise}
      \end{array} \right.
$\\
$
  y_{jt}= \left\{ \begin{array}{ll}
      1 & \textrm{if server $j$ is ON in time slot $t$} \\
      0 & \textrm{otherwise}
      \end{array} \right.
$\\
$z_{jt} \equiv$ speed level of server $j$ in time slot $t$ (e.g. $z_{jt} \in (1/n, 2/n, \ldots, 1)$)\\
$w_{ijt} \equiv$ arrival rate of application $i$ assigned to server $j$ at time $t$\\
\linebreak
\underline{Objective}\\
\begin{eqnarray}
  \min &&  \sum_{j\in S}\sum_{t\in T} y_{jt}(K_j + \alpha_j z_{jt}^3)  \nonumber \\
  s.t.&& \nonumber \\
  &&\sum_{i \in A} \mu_{ij}x_{ij} \le  \mu_j \quad \forall j\in S \label{const_01} \\
  && \sum_{j \in S} x_{ij}y_{jt} \ge 1 \quad \forall i \in A, \forall t \label{const_02} \in T \\
  && \rho \sum_{j\in S} \mu_{ij}x_{ij} y_{jt}z_{jt} \ge \lambda_{it} \quad \forall i\in A, \forall t\in T \label{const_03} \\
  && \sum_{i\in A} \{\rho \mu_{ij} y_{jt} z_{jt} -  w_{ijt}x_{ij}\} \ge 0 \quad \forall j\in   S, \forall t\in T \label{const_04}\\
  && \sum_{j\in S} w_{ijt} \ge \lambda_{it} \quad \forall i\in A, \forall j \in S, \forall t\in T \label{const_05}\\
  && w_{ijt} \ge 0 \quad \forall i\in A, \forall j \in S, \forall t\in T \label{const_06}\\
  && \sum_{i\in A} x_{ij} -y_{jt} \ge 0 \quad \forall j \in S, \forall t\in T \label{const_07}
\end{eqnarray}

\end{document}

